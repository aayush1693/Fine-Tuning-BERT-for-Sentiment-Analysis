# -*- coding: utf-8 -*-
"""fine-tune-bert-sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zjKSxutTopiqn5-s8_JxRKE-oxbKjzne

# **Fine-Tuning BERT Model for Sentiment Analysis**
This notebook demonstrates how to fine-tune a BERT model for sentiment analysis using the Huggingface transformers library. We'll visualize key training metrics like loss and accuracy to understand the model's performance.

1. Install Dependencies

---


First, we install the necessary libraries for working with BERT, datasets, and visualization.
"""

# Install Huggingface's transformers, datasets, and other necessary libraries
!pip install transformers datasets matplotlib

# Import libraries
import pandas as pd
import numpy as np
import torch
import matplotlib.pyplot as plt
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import Trainer, TrainingArguments
from datasets import load_dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

"""2. Load and Prepare Dataset

---


We'll load the SST-2 dataset for sentiment analysis and split it into training and validation sets.
"""

# Load the SST-2 dataset from Huggingface
dataset = load_dataset('glue', 'sst2')

# Split dataset into training and validation
train_data = dataset['train']
test_data = dataset['validation']

# Show an example from the dataset
print(train_data[0])

"""3. Tokenize Data

---
We use the BERT tokenizer to prepare the data for model input.

"""

# Load the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the dataset
def tokenize(batch):
    return tokenizer(batch['sentence'], padding=True, truncation=True)

# Apply tokenization
train_data = train_data.map(tokenize, batched=True, batch_size=len(train_data))
test_data = test_data.map(tokenize, batched=True, batch_size=len(test_data))

# Remove unnecessary columns and set format for PyTorch tensors
train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
test_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

# Check the first tokenized example
print(train_data[0])

"""4. Load Pre-trained BERT Model

---
We now load the pre-trained BERT model with a sequence classification head (for binary classification).

"""

# Load the BERT model for sequence classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

"""5. Define Metrics and Evaluation Function

---


We'll define the accuracy, precision, recall, and F1-score to evaluate the model's performance.
"""

# Define the evaluation metrics
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

"""6. Setup Training Arguments

---
Here, we define the training arguments such as the number of epochs, learning rate, and evaluation strategy.

"""

# Set up the training arguments
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    evaluation_strategy="epoch",     # evaluate each epoch
    num_train_epochs=3,              # number of training epochs
    per_device_train_batch_size=16,  # batch size for training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,                # log every 10 steps
    load_best_model_at_end=True,     # load the best model at the end of training
    save_strategy="epoch",          # Save the model checkpoint at the end of each epoch
)

"""7. Initialize Trainer

---


We initialize the Huggingface Trainer class to handle the training loop and evaluation.
"""

# Initialize the Trainer
trainer = Trainer(
    model=model,                         # the instantiated model
    args=training_args,                  # training arguments
    train_dataset=train_data,            # training dataset
    eval_dataset=test_data,              # evaluation dataset
    compute_metrics=compute_metrics,     # the metrics function
)

"""8. Train the Model and Visualize Training Loss

---
We'll now train the model and capture the training loss at each step for visualization.

"""

# Train the model
train_result = trainer.train()

# Save the model and tokenizer
model.save_pretrained('./fine_tuned_bert')
tokenizer.save_pretrained('./fine_tuned_bert')

# Plot training loss
plt.plot(train_result.training_loss)
plt.title('Training Loss Over Time')
plt.xlabel('Steps')
plt.ylabel('Loss')
plt.show()

"""9. Evaluate the Model


---


After training, we evaluate the model on the validation set to see its performance in terms of accuracy, F1-score, and more.
"""

# Evaluate the model
eval_result = trainer.evaluate()

# Print the evaluation results
print("Evaluation Metrics:")
for key, value in eval_result.items():
    print(f"{key}: {value:.4f}")

"""10. Visualize Training and Evaluation Metrics

---


We'll plot accuracy, precision, recall, and F1-score from the evaluation step
"""

# Store evaluation metrics for visualization
metrics = eval_result

# Define the metric names and values
metric_names = ['accuracy', 'precision', 'recall', 'f1']
metric_values = [metrics['eval_accuracy'], metrics['eval_precision'], metrics['eval_recall'], metrics['eval_f1']]

# Plot the metrics
plt.bar(metric_names, metric_values, color=['blue', 'orange', 'green', 'red'])
plt.title('Evaluation Metrics')
plt.ylim(0, 1)  # Set y-axis limit between 0 and 1 for better readability
plt.show()

"""11. Inference on New Data
We can now use the fine-tuned model to make predictions on new sentences.
"""

# Load the fine-tuned model
fine_tuned_model = BertForSequenceClassification.from_pretrained('./fine_tuned_bert')
fine_tuned_tokenizer = BertTokenizer.from_pretrained('./fine_tuned_bert')

# Perform inference on a new sentence
sentence = "This movie is fantastic!"
inputs = fine_tuned_tokenizer(sentence, return_tensors="pt")
outputs = fine_tuned_model(**inputs)
prediction = torch.argmax(outputs.logits, dim=-1)

# Interpret the prediction
label = 'positive' if prediction == 1 else 'negative'
print(f"Sentiment: {label}")

"""12. Conclusion
In this notebook, we've fine-tuned a BERT model for sentiment analysis, evaluated its performance, and visualized the training loss and evaluation metrics. The model can now be used to classify the sentiment of new sentences.
"""